ğŸ“„ Research Area & Subject Area Prediction for ArXiv Papers






ğŸ“ Project Overview

This project provides a multi-label classification system for predicting research areas and subject areas of scientific papers from ArXiv. It also includes a recommendation engine to suggest similar papers based on titles using Sentence Transformers embeddings.

The project is implemented using:

Deep Learning Models: MLP (TF-IDF), 1D-CNN (embedding + convolution)

Classical ML: Logistic Regression (baseline)

Embedding-based Recommendations: Sentence Transformers

ğŸ”— Dataset

The dataset contains ArXiv paper abstracts and metadata:

Source: Kaggle - ArXiv Paper Abstracts

Columns: titles, abstracts, terms (subject areas)

Preprocessing steps:

Remove duplicate titles

Filter rare terms (appear only once)

Text vectorization using TF-IDF (MLP & Logistic Regression) and integer sequences (1D-CNN)

Multi-label encoding using StringLookup (TensorFlow) and MultiLabelBinarizer (Scikit-learn)

ğŸ›  Features

Subject Area Prediction

Multi-label classification for ArXiv abstracts

Models:

Shallow MLP: TF-IDF vectorized input

Logistic Regression: Baseline, TF-IDF input

1D-CNN: Embedding + Conv1D over sequences

Returns top categories with probabilities

Paper Recommendation

Generates top 5 similar papers based on title embeddings

Uses sentence-transformers (all-MiniLM-L6-v2)

Streamlit Web App

User-friendly interface

Input paper title for recommendations

Input abstract for subject area predictions

Displays predictions with confidence bars

ğŸ“Š Exploratory Data Analysis

Abstract length distribution

Top 20 most frequent subject areas

Word cloud visualization for paper abstracts

Data cleaning & filtering for rare categories

âš™ï¸ Installation

Clone the repository:

git clone https://github.com/<your-username>/arxiv-subject-prediction.git
cd arxiv-subject-prediction


Create a Python virtual environment:

python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows


Install dependencies:

pip install -r requirements.txt


Required packages:

tensorflow==2.15.0

torch==2.0.1

sentence-transformers==2.2.2

streamlit

scikit-learn, pandas, numpy, matplotlib, seaborn, wordcloud

ğŸš€ Usage
1ï¸âƒ£ Jupyter Notebook

Run Arxiv_Subject_Prediction.ipynb for:

Data preprocessing

EDA (plots, word clouds)

Model training (MLP, Logistic Regression, 1D-CNN)

Evaluation and benchmarking

2ï¸âƒ£ Streamlit App

Run the app:

streamlit run app.py


App Features:

Input paper title â†’ get top 5 recommended papers

Input paper abstract â†’ get predicted subject areas with confidence

Displays progress bars and probability scores

ğŸ† Model Architecture
1ï¸âƒ£ Shallow MLP

Input: TF-IDF vector of abstracts

Layers: 512 â†’ 256 â†’ output (sigmoid for multi-label)

Loss: binary_crossentropy

Optimizer: adam

2ï¸âƒ£ Logistic Regression

Baseline linear model

TF-IDF input

One-vs-Rest classification

3ï¸âƒ£ 1D-CNN

Input: Integer-encoded sequences

Embedding layer â†’ Conv1D â†’ GlobalMaxPooling â†’ Dense â†’ Sigmoid

Captures local n-gram patterns

ğŸ“ˆ Results
Model	Accuracy / F1	Notes
Shallow MLP	XX%	Good for frequent categories
Logistic Regression	XX%	Baseline, simple & fast
1D-CNN	XX%	Best performance, captures sequence info

âš  Class imbalance affects rare categories. 1D-CNN performs best on frequent & medium-frequency classes.

ğŸ”® Future Improvements

Fine-tune BERT / SciBERT for state-of-the-art performance

Address class imbalance with oversampling or augmentation

Hyperparameter tuning for CNN / MLP

Deploy web app with Docker / Streamlit Cloud

ğŸ“‚ File Structure
arxiv-subject-prediction/
â”œâ”€ models/                   # Saved models and vectorizers
â”œâ”€ Arxiv_Subject_Prediction.ipynb  # Notebook with preprocessing and training
â”œâ”€ app.py                    # Streamlit app
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ dataset/
   â””â”€ arxiv_data_210930-054931.csv

ğŸ§‘â€ğŸ’» Authors

Ashraf Mahdi

Syed Wasia Ali Shah

ğŸ”— References

Kaggle - ArXiv Paper Abstracts

TensorFlow Docs

Sentence Transformers

Streamlit
