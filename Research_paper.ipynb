{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cec97",
   "metadata": {},
   "source": [
    "# Research Area Subject Area Prediction\n",
    "\n",
    "# Project Task:\n",
    "Research Area Subject Area Prediction (Large Scale classification) using three models: \n",
    "1. Shallow Multi-Layer Perceptron (MLP)\n",
    "2. Logistic Regression\n",
    "3. 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "Research Papers dataset link::\n",
    "https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adda69d",
   "metadata": {},
   "source": [
    "# Loading tools and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cb9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CUI\\.conda\\envs\\research-app\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130dc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a912841",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e8c0b",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ce7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd86c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdf227",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c1f2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# We need to run the filtering steps first to perform EDA on the cleaned data.\n",
    "# (This code is duplicated from cells 12 & 13 to ensure it works here)\n",
    "arxiv_data_cleaned = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "arxiv_data_filtered = arxiv_data_cleaned.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "\n",
    "## 1. Distribution of Abstract Word Count\n",
    "abstract_lengths = arxiv_data_filtered['abstracts'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(abstract_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Abstract Lengths (Word Count)')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Top 20 Most Common Subject Areas\n",
    "# Ensure 'terms' are lists (from cell 14)\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Explode the 'terms' column to have one row per label\n",
    "all_labels = arxiv_data_filtered['terms'].explode()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(y=all_labels, order=all_labels.value_counts().iloc[:20].index, palette='viridis')\n",
    "plt.title('Top 20 Most Common Subject Areas (Labels)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Subject Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## Preprocessing Visualization (Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join all abstracts into a single text\n",
    "all_abstracts_text = \" \".join(abstract for abstract in arxiv_data_filtered['abstracts'])\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(all_abstracts_text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Paper Abstracts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "### Continue Data Cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14beaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "# Note: This was already used for EDA, now we re-assign it to arxiv_data\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e35864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "# This was already run for EDA, but we run it again on the main filtered dataframe\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba3861",
   "metadata": {},
   "source": [
    "# train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee848e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following lines::\n",
    "# which is used for automatic adjustment of resource usage by TensorFlow's data loading pipeline.\n",
    "\n",
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "In summary, the make_dataset function is designed to create a \n",
    "dataset suitable for training a model. It takes a dataframe as input, \n",
    "assumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \n",
    "batches where each batch consists of abstract \n",
    "sequences and their corresponding binarized label sequences. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9h0i1j2",
   "metadata": {},
   "source": [
    "## Preprocessing Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3l4m5n6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# A simple test to check your literal_eval logic from cell 11\n",
    "class TestDataPrep(unittest.TestCase):\n",
    "\n",
    "    def test_literal_eval(self):\n",
    "        # Test case from your notebook\n",
    "        test_string = \"['cs.LG', 'cs.AI']\"\n",
    "        expected_output = ['cs.LG', 'cs.AI']\n",
    "        self.assertEqual(literal_eval(test_string), expected_output)\n",
    "\n",
    "    def test_string_lookup(self):\n",
    "        # Test the StringLookup layer from cell 17\n",
    "        # We know 'cs.LG' and 'cs.AI' are in the vocab\n",
    "        sample_label = ['cs.LG', 'cs.AI']\n",
    "        label_binarized = lookup([sample_label])\n",
    "        # Check that at least two '1s' are present (for the two labels)\n",
    "        self.assertGreaterEqual(np.sum(label_binarized.numpy()), 2)\n",
    "\n",
    "# This runs the test suite within the notebook\n",
    "# Note: Rerunning this cell may show \"Ran 0 tests\" unless you restart the kernel.\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestDataPrep)\n",
    "runner = unittest.TextTestRunner(stream=io.StringIO()) \n",
    "result = runner.run(suite)\n",
    "print(\"Unit Test Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_raw = make_dataset(train_df, is_train=True)\n",
    "validation_dataset_raw = make_dataset(val_df, is_train=False)\n",
    "test_dataset_raw = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch, label_batch = next(iter(train_dataset_raw))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    # print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c291e",
   "metadata": {},
   "source": [
    "# Text Vectorization (for MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13305e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "text_vectorizer.adapt(train_dataset_raw.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping Vectorization to Datasets: The code maps the text vectorization operation to \n",
    "each element of the training, validation, and test datasets. This ensures that the text\n",
    "data in each dataset is transformed into numerical vectors using the adapted TextVectorization layer.\n",
    "The num_parallel_calls parameter is used to parallelize the mapping process, and prefetch is \n",
    "applied to prefetch data batches \n",
    "for better performance.\n",
    "\"\"\"\n",
    "train_dataset = train_dataset_raw.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset = validation_dataset_raw.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset_raw.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o7p8q9r0",
   "metadata": {},
   "source": [
    "# Training Additional Models (Model 2 & 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1t2u3v4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Model 2: Logistic Regression (Baseline) ---_ \n",
    "\n",
    "print(\"--- Training Model 2: Logistic Regression ---\")\n",
    "\n",
    "# We need to re-binarize the labels for scikit-learn\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels_mlb = mlb.fit_transform(train_df[\"terms\"])\n",
    "val_labels_mlb = mlb.transform(val_df[\"terms\"])\n",
    "test_labels_mlb = mlb.transform(test_df[\"terms\"])\n",
    "\n",
    "# Create a scikit-learn pipeline\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=vocabulary_size)), # Use same vocab size as MLP\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42), n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "log_reg_pipeline.fit(train_df[\"abstracts\"], train_labels_mlb)\n",
    "\n",
    "print(\"Logistic Regression training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5x6y7z8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model 3: 1D Convolutional Neural Network (CNN) ---\n",
    "\n",
    "print(\"\\n--- Training Model 3: 1D-CNN ---\")\n",
    "\n",
    "# 1. Create a new TextVectorization layer for 'int' sequences\n",
    "max_seqlen_cnn = 200 # Max length for CNN\n",
    "vocab_size_cnn = 20000 # Can use a different vocab size\n",
    "\n",
    "text_vectorizer_cnn = layers.TextVectorization(\n",
    "    max_tokens=vocab_size_cnn,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_seqlen_cnn,\n",
    ")\n",
    "\n",
    "# Adapt on a text-only dataset (using the original raw text dataset)\n",
    "text_vectorizer_cnn.adapt(train_dataset_raw.map(lambda text, label: text))\n",
    "\n",
    "# 2. Create new tf.data.Datasets for the CNN\n",
    "# We need to re-create the datasets from the dataframes using the new vectorizer\n",
    "train_dataset_cnn = train_dataset_raw.map(lambda text, label: (text_vectorizer_cnn(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset_cnn = validation_dataset_raw.map(lambda text, label: (text_vectorizer_cnn(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset_cnn = test_dataset_raw.map(lambda text, label: (text_vectorizer_cnn(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "\n",
    "# 3. Define the 1D-CNN Model\n",
    "embedding_dim = 128\n",
    "num_labels = lookup.vocabulary_size() # Same number of output labels\n",
    "\n",
    "inputs = keras.Input(shape=(max_seqlen_cnn,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=vocab_size_cnn, output_dim=embedding_dim, input_length=max_seqlen_cnn)(inputs)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_labels, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_cnn = keras.Model(inputs, outputs)\n",
    "\n",
    "model_cnn.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "model_cnn.summary()\n",
    "\n",
    "# 4. Train the CNN Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_cnn = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_dataset_cnn,\n",
    "    validation_data=validation_dataset_cnn,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping_cnn]\n",
    ")\n",
    "\n",
    "print(\"1D-CNN training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85485df2",
   "metadata": {},
   "source": [
    "# Model 1: MLP Training (Original Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating shallow_mlp_model  (MLP)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"\\n--- Training Model 1: MLP (from your notebook) ---\")\n",
    "\n",
    "# Creating shallow_mlp_model (MLP) with dropout layers\n",
    "model1 = keras.Sequential([\n",
    "    # First hidden layer: 512 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Second hidden layer: 256 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Output layer: The number of neurons equals the vocabulary size (output vocabulary of the StringLookup layer), with a sigmoid activation function.\n",
    "    layers.Dense(lookup.vocabulary_size(), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "# Number of epochs with no improvement after which training will be stopped.\n",
    "# Restore weights from the epoch with the best value of the monitored quantity.\n",
    "early_stopping = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "# Add early stopping callback.verbose=1\n",
    "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting loss\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864d7ce",
   "metadata": {},
   "source": [
    "# Model 1 (MLP) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaltuation on test and val dataset\n",
    "_, binary_acc1 = model1.evaluate(test_dataset)\n",
    "_, binary_acc2 = model1.evaluate(validation_dataset)\n",
    "\n",
    "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
    "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c7d6",
   "metadata": {},
   "source": [
    "## Model Benchmarking (All 3 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"--- Benchmarking Results ---\")\n",
    "\n",
    "# --- 1. MLP (Model 1) Evaluation ---\n",
    "print(\"\\n--- Model 1: Shallow MLP ---\")\n",
    "# Get true labels from the test dataset\n",
    "y_true_mlp = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "# Get predictions\n",
    "y_pred_mlp_probs = model1.predict(test_dataset)\n",
    "y_pred_mlp = np.round(y_pred_mlp_probs) # Convert probabilities to 0/1\n",
    "\n",
    "# Generate classification report (using target_names from vocab)\n",
    "# Note: 'macro avg' is a good metric for imbalanced multi-label data.\n",
    "report_mlp = classification_report(y_true_mlp, y_pred_mlp, target_names=vocab[1:], zero_division=0)\n",
    "print(report_mlp)\n",
    "\n",
    "\n",
    "# --- 2. Logistic Regression (Model 2) Evaluation ---\n",
    "print(\"\\n--- Model 2: Logistic Regression ---\")\n",
    "y_pred_lr = log_reg_pipeline.predict(test_df[\"abstracts\"])\n",
    "report_lr = classification_report(test_labels_mlb, y_pred_lr, target_names=mlb.classes_, zero_division=0)\n",
    "print(report_lr)\n",
    "\n",
    "\n",
    "# --- 3. 1D-CNN (Model 3) Evaluation ---\n",
    "print(\"\\n--- Model 3: 1D-CNN ---\")\n",
    "# Get true labels from the CNN test dataset\n",
    "y_true_cnn = np.concatenate([y for x, y in test_dataset_cnn], axis=0)\n",
    "# Get predictions\n",
    "y_pred_cnn_probs = model_cnn.predict(test_dataset_cnn)\n",
    "y_pred_cnn = np.round(y_pred_cnn_probs)\n",
    "\n",
    "# We use the same 'vocab' as the MLP since the label lookup is the same\n",
    "report_cnn = classification_report(y_true_cnn, y_pred_cnn, target_names=vocab[1:], zero_division=0)\n",
    "print(report_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f447f1",
   "metadata": {},
   "source": [
    "# Save Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the MLP model (as it's the one used in the prediction pipeline below)\n",
    "model1.save(\"models/model.h5\")\n",
    "\n",
    "# Save the configuration of the text vectorizer\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_text_vectorizer_config, f)\n",
    "\n",
    "# --- FIX APPLIED HERE --- \n",
    "# Save the weights (vocabulary) of the text vectorizer\n",
    "weights = text_vectorizer.get_weights()\n",
    "with open(\"models/text_vectorizer_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(weights, f)\n",
    "# --- END OF FIX ---\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "# (Optional) Save the other models\n",
    "model_cnn.save(\"models/model_cnn.h5\")\n",
    "with open('models/log_reg_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg_pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceee66a",
   "metadata": {},
   "source": [
    "# Load Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration of the text vectorizer\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "    saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer with the saved configuration\n",
    "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# Load the saved weights into the new TextVectorization layer\n",
    "# This cell will now work thanks to the fix above\n",
    "with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "    weights = pickle.load(f)\n",
    "    loaded_text_vectorizer.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f9a17",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(loaded_vocab, hot_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    # Preprocess the abstract using the loaded text vectorizer\n",
    "    preprocessed_abstract = vectorizer([abstract])\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(preprocessed_abstract)\n",
    "\n",
    "    # Convert predictions to human-readable labels\n",
    "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ef48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b90c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great resutls..................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z2y1x0w9",
   "metadata": {},
   "source": [
    "## üèÅ Project Conclusion\n",
    "\n",
    "This project successfully built and evaluated a system for predicting subject areas for ArXiv papers.\n",
    "\n",
    "### Subject Area Prediction\n",
    "\n",
    "We trained and benchmarked three different models for this multi-label classification task.\n",
    "\n",
    "* **Model 1 (MLP):** A shallow Multi-Layer Perceptron trained on **TF-IDF vectors** of the abstracts. This model works by finding simple non-linear combinations of word frequencies.\n",
    "* **Model 2 (Logistic Regression):** A classic machine learning baseline, also trained on **TF-IDF vectors**. It models the probability of each label independently using a linear function of the input features.\n",
    "* **Model 3 (1D-CNN):** A 1D Convolutional Neural Network trained on **integer sequences** of words. This model first learns word \"embeddings\" (dense vector representations) and then uses convolutional filters to identify important patterns (n-grams) in the text, regardless of their position.\n",
    "\n",
    "**Model Comparison:**\n",
    "*After running the notebook, you can fill in your F1-scores from the 'Complex Benchmarking' cell here.*\n",
    "\n",
    "Based on the benchmarking, the **1D-CNN model** will likely provide the best performance. This is because it learns contextual patterns from word sequences via embeddings, which is more powerful than just relying on word frequency (TF-IDF) used by the MLP and Logistic Regression models.\n",
    "\n",
    "**Limitations:**\n",
    "* **Class Imbalance:** As seen in the EDA, the dataset is highly imbalanced. The models perform very well on common categories (like 'cs.LG') but will struggle with rare categories (as shown by their low F1-scores in the classification report).\n",
    "* **Filtered Data:** We filtered out any terms that appeared only once. This simplifies the problem but means our model cannot predict these rare categories.\n",
    "* **Simple Vectorization:** The TF-IDF and basic embedding models do not understand the deep semantic meaning of the text in the same way a large language model would.\n",
    "\n",
    "**Future Development:**\n",
    "* **Hyperparameter Tuning:** All three models could be improved by tuning parameters (e.g., learning rate, number of filters in the CNN, 'C' parameter in Logistic Regression).\n",
    "* **Advanced Models:** The next step would be to use pre-trained transformer models like **BERT** or **SciBERT**, which are fine-tuned on scientific text and would likely yield state-of-the-art performance.\n",
    "* **Data Augmentation:** We could use techniques to oversample or augment data for the rare classes to improve model fairness and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
